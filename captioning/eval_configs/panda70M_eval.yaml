model:
  arch: video_llama
  model_type: pretrain_vicuna
  input_prompt: True
  ckpt: "checkpoint/checkpoint_best.pth"

  # Q-Former
  num_query_token: 32

  # Vicuna
  llama_model: "vicuna_weights/vicuna-7b-v0"
  
  # Branch  
  fusion_head_layers: 2
  max_frame_pos: 32
  fusion_header_type: "seqTransf"
  num_video_query_token: 32
  num_text_query_token: 32
  input_vid2tex_query_embed: True
  detach_video_query_embed: True

  max_caption_len: 48
  max_prompt_len: 200
  start_sym: "<s>"
  end_sym: "</s>"

datasets:
  hdvila:
    vis_processor:
      train:
        name: "alpro_video_eval"
        n_frms: 8
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
        
run:
  task: video_text_pretrain
